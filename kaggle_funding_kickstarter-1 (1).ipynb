{"cells":[{"cell_type":"markdown","source":["# Funding Successfull Projects on Kickstarter\n*Resources: https://www.kaggle.com/codename007/funding-successful-projects\n\n*Data: https://www.kaggle.com/codename007/funding-successful-projects/data"],"metadata":{}},{"cell_type":"markdown","source":["# Objective \n\n* Kickstarter is a community of more than 10 million people comprising of creative, tech enthusiasts who help in bringing creative project to life. Till now, more than $3 billion dollars have been contributed by the members in fueling creative projects. The projects can be literally anything – a device, a game, an app, a film etc.\n\n* Kickstarter works on all or nothing basis i.e., if a project doesn’t meet its goal, the project owner gets nothing. \n* For example: if a project’s goal is $500. Even if it gets funded till $499, the project won’t be a success.\n\n* In this challange, we will try to predict whether a project will get successfully funded or not."],"metadata":{}},{"cell_type":"markdown","source":["# Dataset:\n* For this project we will be using the dataset obtained from Kaggle and is in the link \n* https://www.kaggle.com/codename007/funding-successful-projects/data"],"metadata":{}},{"cell_type":"code","source":["%sh\n# Right click on Kaggle Download button to get link information\n# Next, place on a shared drive (below GSU OneDrive), and get link to the shared drive\n# Placed the link below:\nwget \"https://mygsu-my.sharepoint.com/personal/wrobinson_gsu_edu/_layouts/15/guestaccess.aspx?docid=0e3b03f3c7df34344a9db478adce1e984&authkey=ARnwjDE8SCQ3TMRofgoIE2g&e=1883debab986481889ef02ea44d76f2a\"  -O kickstarter.zip; unzip kickstarter.zip"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["%sh\nls /databricks/driver"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["train_df = spark.read.load(\"file:/databricks/driver/train.csv\",\\\n                    format='com.databricks.spark.csv',\\\n                    header='true',\\\n                    mode = 'DROPMALFORMED',\\\n                    inferSchema='true')\n\ntrain_df.printSchema()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["display(train_df)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["train_df.count()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["test_df = spark.read.load(\"file:/databricks/driver/test.csv\",\\\n                    format='com.databricks.spark.csv',\\\n                    header='true',\\\n                    mode = 'DROPMALFORMED',\\\n                    inferSchema='true')\n\ntest_df.printSchema()"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["display(test_df)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["test_df.count()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["# DATA Transforming and Cleaning\n* Converting unixtime into YYYY-MM--DD HH:MM:SS\n* Removing null rows"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import *\nfrom pyspark.sql.functions import to_date\nimport datetime, time\nfrom pyspark.sql.functions import UserDefinedFunction\n\ndef toTimeStamp(s):\n  # If you have milliseconds then divide by 1000.0\n  if (s):\n    return time.strftime('%Y-%m-%d %H:%M:%S', time.gmtime(long(s)/1.0))\n  else:\n    return s\ntoTimeStamp_UDF = UserDefinedFunction(toTimeStamp, StringType())\n\n# Converting column data types\ndf = train_df\ndf = df.withColumn(\"goal\", df[\"goal\"].cast(IntegerType()))\ndf = df.withColumn(\"backers_count\", df[\"backers_count\"].cast(IntegerType()))\ndf = df.withColumn(\"final_status\", df[\"final_status\"].cast(IntegerType()))\n\n# Readable dates...\ndf = df.withColumn(\"dealineDT\", toTimeStamp_UDF(df.deadline))\ndf = df.withColumn(\"created_atDT\", toTimeStamp_UDF(df.created_at))\ndf = df.withColumn(\"launched_atDT\", toTimeStamp_UDF(df.launched_at))\ndf = df.withColumn(\"state_changed_atDT\", toTimeStamp_UDF(df.state_changed_at))\n\n\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["display(train_df)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["from pyspark.sql.types import *\nfrom pyspark.sql.functions import to_date\nimport datetime, time\nfrom pyspark.sql.functions import UserDefinedFunction\n\ndef toTimeStamp(s):\n  # If you have milliseconds then divide by 1000.0\n  if (s):\n    return time.strftime('%Y-%m-%d %H:%M:%S', time.gmtime(long(s)/1.0))\n  else:\n    return s\ntoTimeStamp_UDF = UserDefinedFunction(toTimeStamp, StringType())\n\n# Converting column data types\ndf1 = test_df\ndf1 = df1.withColumn(\"goal\", df1[\"goal\"].cast(IntegerType()))\n\n# Readable dates...\ndf1 = df1.withColumn(\"dealineDT\", toTimeStamp_UDF(df1.deadline))\ndf1 = df1.withColumn(\"state_changed_atDT\", toTimeStamp_UDF(df1.state_changed_at))\ndf1 = df1.withColumn(\"created_atDT\", toTimeStamp_UDF(df1.created_at))\ndf1 = df1.withColumn(\"launched_atDT\", toTimeStamp_UDF(df1.launched_at))\n\n\ndisplay(df1)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["trainCleaned = df.filter(df.project_id.isNotNull() & df.name.isNotNull() & df.desc.isNotNull())\ndisplay(trainCleaned)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["trainCleaned.count()"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["testCleaned = df1.filter(df1.project_id.isNotNull() & df1.desc.isNotNull())\ndisplay(testCleaned)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["testCleaned.count()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["#Pattern Discovery and Model Evaluation"],"metadata":{}},{"cell_type":"code","source":["trainCleaned = df.filter(df.project_id.isNotNull() & df.name.isNotNull() & df.desc.isNotNull())\ndisplay(trainCleaned)\n"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["* As you can see only Only 30% of Projects are granted."],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["trainCleaned = df.filter(df.project_id.isNotNull() & df.name.isNotNull() & df.desc.isNotNull())\ndisplay(trainCleaned)\n#Below is the graph for disable communication"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["trainCleaned.groupBy('disable_communication').count().show()"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["trainCleaned.registerTempTable(\"Train_temp\")"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["%sql\nSELECT final_status,\nsum(CASE WHEN goal>= 0 AND goal < 10000 THEN 1 END) AS  G10K,\nsum(CASE WHEN goal>= 10000 AND goal < 50000 THEN 1 END) AS G10Kto50K,\nsum(CASE WHEN goal>= 50000 AND goal < 100000 THEN 1 END) AS G50Kto100K,\nsum(CASE WHEN goal>= 100000 AND goal < 1000000 THEN 1 END) AS G100Kto1M,\nsum(CASE WHEN goal>= 1000000 THEN 1 END) AS G1M\nFROM Train_temp as Goalgroups\ngroup by final_status"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["%sql\nselect log(goal) as log_goal from Train_temp"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["%sql\nselect backers_count, final_status, log(goal) as log_goal from Train_temp"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["* Average Goal value when Funding is not approved"],"metadata":{}},{"cell_type":"code","source":["%sql\nselect avg(goal) from Train_temp where final_status =0 group by final_status\n"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["* Average Goal value when Funding is approved"],"metadata":{}},{"cell_type":"code","source":["%sql\nselect avg(goal) from Train_temp where final_status =1 group by final_status"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["* The mean is high for successful Funding. And For 100% Successful funding \"disable_communication\" is False."],"metadata":{}},{"cell_type":"code","source":["%sql\nselect final_status, goal, disable_communication from Train_temp"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["%sql\nselect country, final_status from Train_temp"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["%sql\nselect currency, country,final_status from Train_temp"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":["* As you can see Most of the the application for Funding come from US, GB, Canada and Austrailia."],"metadata":{}},{"cell_type":"code","source":["%sql\nselect backers_count,final_status from Train_temp"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":["#We can easily conclude that backers_count is high for successfully funded projects."],"metadata":{}}],"metadata":{"name":"kaggle_funding_kickstarter-1 (1)","notebookId":781348748973659},"nbformat":4,"nbformat_minor":0}
